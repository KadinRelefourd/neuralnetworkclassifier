{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KadinRelefourd/neuralnetworkclassifier/blob/main/classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW7iqcpe78L1"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vqlwu1Uu535l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2  # OpenCV for image processing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFVW_Z6a-Gby"
      },
      "source": [
        "Prepare Training and Testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YA3Ueyp-LAy",
        "outputId": "6f37e012-93fe-4eba-c5f2-cea70a03c761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(\n",
        "    root='./data.cifar10',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root='./data.cifar10',\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaw4gBlL-MrO"
      },
      "source": [
        "Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nmtGhK8Q-Tvd"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # CIFAR-10 images are 32x32x3\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        # Output size is 10 because of the 10 classes in CIFAR-10\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten the tensor becuase the linear layer only accepts a vector.\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #used relu as the activation function\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        #I applied batc norm for better training time and applied dropout\n",
        "        # but had to lower the probability of dropout to increase accuracy\n",
        "        # maybe because I have 3 layers instead of 2\n",
        "        x = F.dropout(x, p=0.3)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = F.dropout(x, p=0.3)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = F.dropout(x, p = 0.3)\n",
        "        x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "#crossEntropyLoss includes softmax and is the loss function you need for\n",
        "#classification problems\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "#used SGD because adam gave me low accuracy and played with the hyperparametss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfJg-NnS-WC4"
      },
      "source": [
        "Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s7sAIYwp-a55"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    # switches the model to evaluation mode by disabling dropout and bn\n",
        "    model.eval()\n",
        "    #these will be used for accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    #loss accumilation\n",
        "    total_loss = 0.0\n",
        "    # Disable gradient computation (saves memory and speeds up inference)\n",
        "    with torch.no_grad():\n",
        "        #x is the input and y is the target\n",
        "        for (x, y) in test_loader:\n",
        "            #get prediciotns from the model\n",
        "            outputs = model(x)\n",
        "            # loss for the batch\n",
        "            loss = loss_func(outputs, y)  # Compute the loss for the batch\n",
        "            #loss accumulation\n",
        "            total_loss += loss.item()\n",
        "            #this is determining the predicted class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "             # total number of samples\n",
        "            total += y.size(0)\n",
        "            # number of correct samples total\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "    #accuracy percentage\n",
        "    accuracy = 100 * correct / total  # Compute the accuracy percentage\n",
        "    # return average lostt and accuracy\n",
        "    return total_loss / len(test_loader), accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Accuracy"
      ],
      "metadata": {
        "id": "ENP9HQ7qlVFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_accuracy():\n",
        "    #test accuracy and train accuracy are diff so diff functionts\n",
        "    #but same idea as what's in the test function\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "whfg7sddlW_9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WZn7rdkC-7Qt"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    # loads model for classification\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuxnBZYZ-pLb"
      },
      "source": [
        "Save Model function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ygU-J95x-zU8"
      },
      "outputs": [],
      "source": [
        "def save_model(epoch):\n",
        "    #saves model in the model folder in model.ckpt\n",
        "    if not os.path.exists('./model'):\n",
        "        os.makedirs('./model')\n",
        "    torch.save(model.state_dict(), './model/model.ckpt')\n",
        "    print(f\"Model saved in file: ./model/model.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjFxIcLZ-3Zs"
      },
      "source": [
        "Load Model Funciont"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxTosHPZ-_PF"
      },
      "source": [
        "Train Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HQtCdVJC_Jxv"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    #the header\n",
        "    print(f\"{'Loop':<8}{'Train Loss':<15}{'Train Acc %':<15}{'Test Loss':<15}{'Test Acc %':<15}\")\n",
        "    #tracks training acc\n",
        "    last_train_acc = 0\n",
        "\n",
        "    # trains for 10 epochs\n",
        "    for epoch in range(10):\n",
        "        # sets model to trianing mode\n",
        "        model.train()\n",
        "        #counts accumulated loss\n",
        "        total_loss = 0.0\n",
        "        #iterates over traing data x is the inputs and y is ground truth\n",
        "        for x, y in train_loader:\n",
        "            #clear the gradient from the last batch so no old gradients are used\n",
        "            optimizer.zero_grad()\n",
        "            #forward pass\n",
        "            outputs = model(x)\n",
        "            #loss computation\n",
        "            loss = loss_func(outputs, y)\n",
        "            #back propogation\n",
        "            loss.backward()\n",
        "            #this is what updates, it's based on the gradient\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # updates after each epoch\n",
        "        train_acc = train_accuracy()\n",
        "        test_loss, test_accuracy = test()\n",
        "\n",
        "        # makes training accuracy only goes up\n",
        "        if train_acc < last_train_acc:\n",
        "         last_train_acc = train_acc\n",
        "\n",
        "        # prints out results of each epoch\n",
        "        print(f\"{epoch + 1}/10   {total_loss / len(train_loader):<15.4f}{train_acc:<15.2f}{test_loss:<15.4f}{test_accuracy:<15.2f}\")\n",
        "\n",
        "        # Save the model only after the last epoch (10th epoch)\n",
        "        if epoch == 9:\n",
        "            save_model(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5xReWaR_NNx"
      },
      "source": [
        "Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "U7OcjPEm_U7d"
      },
      "outputs": [],
      "source": [
        "def classify(image_path):\n",
        "    # loads the best model from training\n",
        "    load_model('./model/model.ckpt')\n",
        "    model.eval()\n",
        "\n",
        "    # read the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(\"Error: Image not found or invalid path.\")\n",
        "        return\n",
        "    # Convert black/white back to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    #image resizing to make sure it's 32x32\n",
        "    resized = cv2.resize(img, (32, 32))\n",
        "    # Convert to tensor and makes it have one channel\n",
        "    tensor = transform(resized).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #forward pass\n",
        "        output = model(tensor)\n",
        "        #prediction of class\n",
        "        _, predicted_class = torch.max(output.data, 1)\n",
        "    #which classes it can choose from\n",
        "    classes = train_data.classes\n",
        "    #prediction results\n",
        "    print(f'Prediction result: {classes[predicted_class.item()]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu_4nh7o_b7S"
      },
      "source": [
        "Command Line prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vA1MWw0x_a-9"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    command = sys.argv[1]\n",
        "    # trains the data to create model \"python classify.py train\"\n",
        "    if command == \"train\":\n",
        "        train()\n",
        "    # can predict class when given image in class path \"python classify.py test ./filepath\"\n",
        "    elif command == \"test\":\n",
        "        image_path = sys.argv[2]\n",
        "        classify(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6epvE1AYoZ"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python classify.py train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5HPNlUpn6CF",
        "outputId": "4ea77c18-5866-45ff-d2c3-def44a145dbe"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Loop    Train Loss     Train Acc %    Test Loss      Test Acc %     \n",
            "1/10   1.9426         37.25          1.7603         37.11          \n",
            "2/10   1.7045         40.56          1.6785         39.77          \n",
            "3/10   1.6216         43.56          1.6032         43.13          \n",
            "4/10   1.5681         44.91          1.5933         43.27          \n",
            "5/10   1.5234         46.68          1.5427         45.10          \n",
            "6/10   1.4880         47.59          1.5220         45.52          \n",
            "7/10   1.4577         48.95          1.5015         46.38          \n",
            "8/10   1.4310         49.80          1.4929         46.87          \n",
            "9/10   1.4077         50.25          1.4964         47.45          \n",
            "10/10   1.3864         51.96          1.4485         48.58          \n",
            "Model saved in file: ./model/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "sBAP_av1n--Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python classify.py test catfromcifar.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX3DnZx8oBo4",
        "outputId": "777b2c55-e7f9-4b7f-f477-68496ea73513"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/content/classify.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "Prediction result: ship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOST RECENT 2/20/2025"
      ],
      "metadata": {
        "id": "FBqINUXUzK9X"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMpaNFvhvpGECgIwVCtfTB2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}